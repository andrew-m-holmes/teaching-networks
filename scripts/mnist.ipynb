{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1ac3d0-b01d-4597-ba97-004d58b85316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28583600-2e6f-4759-9e9f-c5b087ef2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_dataset(\"mnist\")\n",
    "train, test = mnist.get(\"train\"), mnist.get(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6693dc6e-5cce-49af-8072-89ddd7c983f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_format(type=\"numpy\", columns=[\"image\", \"label\"])\n",
    "test.set_format(type=\"numpy\", columns=[\"image\", \"label\"])\n",
    "num_train_samples = 10000\n",
    "num_test_samples = 1000\n",
    "\n",
    "train_indices = np.random.choice(num_train_samples, num_train_samples, replace=False)\n",
    "test_indices = np.random.choice(num_test_samples, num_test_samples, replace=False)\n",
    "train = train.rename_column(\"image\", \"input\").select(train_indices)\n",
    "test = test.rename_column(\"image\", \"input\").select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe26ee2-d4aa-4a73-965f-7e61a2cdc8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3676d3026f2a4840827abf6ce7eea084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20ec657681e4bcc9ca5099ded359895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    arr = np.reshape(example[\"input\"], -1)\n",
    "    example[\"input\"] = arr\n",
    "    return example\n",
    "\n",
    "train = train.map(preprocess, num_proc=2)\n",
    "test = test.map(preprocess, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39dfbf84-963a-4231-acc6-b24ed0e5df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_inputs = torch.from_numpy(train[\"input\"]).float().squeeze()\n",
    "test_inputs = torch.from_numpy(test[\"input\"]).float().squeeze()\n",
    "train_labels = torch.from_numpy(train[\"label\"]).long()\n",
    "test_labels = torch.from_numpy(test[\"label\"]).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47fd4ac4-b049-492c-884e-afc8064dd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(train_inputs, train_labels)\n",
    "test_dataset = data.TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dd86ed-fb34-4c30-bc3f-8912dab76e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(28 * 28, 512)\n",
    "        self.norm_1 = nn.LayerNorm(512)\n",
    "        self.drop_1 = nn.Dropout(p=0.4)\n",
    "        self.linear_2 = nn.Linear(512, 512)\n",
    "        self.norm_2 = nn.LayerNorm(512)\n",
    "        self.drop_2 = nn.Dropout(p=0.2)\n",
    "        self.linear_3 = nn.Linear(512, 512)\n",
    "        self.norm_3 = nn.LayerNorm(512)\n",
    "        self.linear_4 = nn.Linear(512, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop_1(f.relu(self.norm_1(self.linear_1(x))))\n",
    "        x = self.drop_2(f.relu(self.norm_2(self.linear_2(x))))\n",
    "        x = f.relu(self.norm_3(self.linear_3(x)))\n",
    "        out = self.linear_4(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd2d913-8c03-4247-8aec-23d992e7d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size):\n",
    "    return data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "def train(model, optimizer, train_dataloader, test_dataloader, epochs=10, device=None, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Training has started\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        test_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if verbose and epoch and ((epoch + 1) % int(epochs * 0.25) == 0):\n",
    "            print(f\"Epoch {epoch + 1} complete, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Training is complete\")\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388dbdc3-2cf6-4187-8c0a-9dd809380f4e",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d552508-7ac6-41ea-9f33-0e51bb672f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=len(train_dataset))\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6826022-8e5c-4749-9b05-88a1c5df5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 935434\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce172db-fe01-4b15-90b0-5e8438931975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.666, test loss: 0.643\n",
      "Epoch 50 complete, train loss: 0.346, test loss: 0.364\n",
      "Epoch 75 complete, train loss: 0.254, test loss: 0.297\n",
      "Epoch 100 complete, train loss: 0.201, test loss: 0.260\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e7f284-0d12-4166-8dbb-85cfec2585e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./batch_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)\n",
    "\n",
    "with open(\"./batch_time.pkl\", \"wb\") as file:\n",
    "    pickle.dump(elapsed_time, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a467e96d-ae83-4932-865d-b96d06aa9ea2",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760b1267-804e-4c8f-8ede-2a3c8c112381",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=512)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2926c068-6618-4ca8-8bd2-349e2c054edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.066, test loss: 0.159\n",
      "Epoch 50 complete, train loss: 0.022, test loss: 0.158\n",
      "Epoch 75 complete, train loss: 0.013, test loss: 0.163\n",
      "Epoch 100 complete, train loss: 0.007, test loss: 0.178\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e45569-5675-4c37-b46e-5e71295eaf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mini_batch_512_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)\n",
    "\n",
    "with open(\"./mini_batch_512_time.pkl\", \"wb\") as file:\n",
    "    pickle.dump(elapsed_time, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191a6cb-5327-40e8-9334-c8390ef1aa9a",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c61d7d8d-7e14-416f-aa82-bc985709d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=256)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcb96d9c-85a0-482b-ac7c-2bfc8cd5c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.109, test loss: 0.741\n",
      "Epoch 50 complete, train loss: 0.015, test loss: 0.144\n",
      "Epoch 75 complete, train loss: 0.009, test loss: 0.238\n",
      "Epoch 100 complete, train loss: 0.008, test loss: 0.157\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe008914-dff3-49d8-96d5-21e38890d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mini_batch_256_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e38f9-02aa-4fff-bb43-88c7f802c45d",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18c335e6-8c04-4034-8162-f893f85bba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=128)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b2483e-6965-43e8-821f-0f7b942f765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.040, test loss: 0.202\n",
      "Epoch 50 complete, train loss: 0.009, test loss: 0.151\n",
      "Epoch 75 complete, train loss: 0.011, test loss: 0.163\n",
      "Epoch 100 complete, train loss: 0.006, test loss: 0.241\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a6c5a5e-7687-4186-b9bc-d33a495891e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mini_batch_128_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430012a-00f1-44fd-a467-168e5790331c",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a9255e-877f-4aed-89c1-7315b8c1eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=64)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59a02584-26f9-40bb-a523-f1b6bba96728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.035, test loss: 0.224\n",
      "Epoch 50 complete, train loss: 0.012, test loss: 0.172\n",
      "Epoch 75 complete, train loss: 0.009, test loss: 0.168\n",
      "Epoch 100 complete, train loss: 0.004, test loss: 0.145\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27d9d538-e49d-4016-aa86-b7f7840f406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mini_batch_64_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c1316-9b1e-4c8f-ba89-4bb02efbb50d",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1a24949-3c4c-4016-ada9-6384350cc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=32)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5289e643-7ebf-433d-8bee-d453938b6ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.036, test loss: 0.143\n",
      "Epoch 50 complete, train loss: 0.008, test loss: 0.163\n",
      "Epoch 75 complete, train loss: 0.006, test loss: 0.177\n",
      "Epoch 100 complete, train loss: 0.003, test loss: 0.168\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2757495-80ba-4616-8b14-0e3096f1219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mini_batch_32_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122cc2bc-42db-459e-b820-9f8a2d2b5fce",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c4c29a-942b-4adb-8fec-9e57ced80ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, batch_size=1)\n",
    "test_dataloader = create_dataloader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b9dc517-bb8d-4b89-9da9-80cf195f3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has started\n",
      "Epoch 25 complete, train loss: 0.058, test loss: 0.155\n",
      "Epoch 50 complete, train loss: 0.033, test loss: 0.213\n",
      "Epoch 75 complete, train loss: 0.024, test loss: 0.207\n",
      "Epoch 100 complete, train loss: 0.014, test loss: 0.220\n",
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_loss, test_loss = train(model, optimizer, train_dataloader, test_dataloader, epochs=100, device=device, verbose=True)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9a17f39-e530-4780-a2bc-8257f04a7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stochastic_metrics.pkl\", \"wb\") as file:\n",
    "    pickle.dump((train_loss, test_loss), file)\n",
    "\n",
    "with open(\"./stochastic_time.pkl\", \"wb\") as file:\n",
    "    pickle.dump(elapsed_time, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
